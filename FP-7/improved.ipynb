{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata":  {},
   "source": [
    "# machine.ipynb\n",
    "\n",
    "Machine learning analysis to predict student stress levels.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Changes from FP-6\n",
    "\n",
    "**Fixes from earlier session (correcting FP-6 mistakes):**\n",
    "- Used 5-fold cross-validation instead of single train/test split\n",
    "- Added hyperparameter tuning for both models\n",
    "- Aggregated confusion matrices across all folds\n",
    "\n",
    "**New improvements for FP-7 (preprocessing & dimensionality reduction):**\n",
    "- Applied **StandardScaler** to normalize features before KNN (Lesson 13)\n",
    "- Applied **PCA** to reduce dimensionality and remove correlated features (Lesson 14)\n",
    "- Imported data from parse_data.ipynb instead of loading CSV directly\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Plan\n",
    "\n",
    "**Research Question:** Can we predict a student's stress level from their daily habits?\n",
    "\n",
    "**Data:**\n",
    "- Target (y): stress (Low=1, Moderate=2, High=3)\n",
    "- Features (X): studyhours, sleephours, socialhours, activityhours, Gender\n",
    "\n",
    "**Preprocessing (new for FP-7):**\n",
    "- StandardScaler:  transforms features to mean=0, std=1\n",
    "- PCA: reduces to components explaining 90%+ variance\n",
    "\n",
    "**Models:** KNN and Decision Tree with 5-fold cross-validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs":  [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 'parse_data.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender not in parse_data.ipynb, so add it here\n",
    "df0 = pd.read_csv('lifestylestudents.csv')\n",
    "df['Gender'] = df0['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "X = df[['studyhours', 'sleephours', 'socialhours', 'activityhours', 'Gender']]\n",
    "y = df['stress']\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "The dataset contains 2000 students.  Stress levels are distributed as:\n",
    "- Low:  ~24%\n",
    "- Moderate:  ~24%\n",
    "- High:  ~51%\n",
    "\n",
    "Classes are somewhat imbalanced but not severely.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:  Scaling\n",
    "\n",
    "KNN uses Euclidean distance to find nearest neighbors. Without scaling, features with larger ranges dominate the distance calculation.\n",
    "\n",
    "Our features have different ranges:\n",
    "- studyhours: 5-10\n",
    "- sleephours:  5-10\n",
    "- socialhours: 0-6\n",
    "- activityhours: 0-13\n",
    "- Gender: 0-1\n",
    "\n",
    "StandardScaler transforms each feature to mean=0 and std=1, so all features contribute equally.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "After scaling, all features have mean ≈ 0 and std = 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Dimensionality Reduction:  PCA\n",
    "\n",
    "PCA finds directions of maximum variance in the data. If features are correlated, fewer components can explain most of the variance.\n",
    "\n",
    "From FP-6, we know study hours and sleep hours dominate stress prediction.  PCA should confirm this.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26, 0.47, 0.66, 0.84, 1.0])"
      ]
     },
     "execution_count":  5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source":  [
    "pca_full = PCA(n_components=5)\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "np.round(cumulative_variance, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata":  {},
   "source": [
    "Cumulative variance explained:\n",
    "- PC1: 26%\n",
    "- PC1-2: 47%\n",
    "- PC1-3: 66%\n",
    "- PC1-4: 84%\n",
    "- PC1-5: 100%\n",
    "\n",
    "We need 4 components to reach 84%, or all 5 for 90%+.  This suggests the features in this dataset are not highly correlated with each other (unlike the FP-6 feature importance results suggested for the Decision Tree).\n",
    "\n",
    "We'll test both scaled data and PCA-reduced data to see which works better.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "---\n",
    "\n",
    "## Parameter Tuning\n",
    "\n",
    "Testing different hyperparameters to find optimal settings, now using scaled data for KNN.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=\"1\"><thead><tr><th>n_neighbors</th><th>Mean Accuracy</th></tr></thead><tbody><tr><td>3</td><td>90.25%</td></tr><tr><td>5</td><td>91.10%</td></tr><tr><td>7</td><td>91.55%</td></tr><tr><td>9</td><td>91.85%</td></tr><tr><td>11</td><td>92.15%</td></tr><tr><td>15</td><td>91.90%</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KNN tuning with scaled features\n",
    "knn_results = []\n",
    "for k in [3, 5, 7, 9, 11, 15]:\n",
    "    scores = cross_val_score(KNeighborsClassifier(n_neighbors=k), X_scaled, y, cv=kfold)\n",
    "    knn_results.append({'n_neighbors': k, 'mean':  scores.mean(), 'std': scores. std()})\n",
    "\n",
    "knn_df = pd.DataFrame(knn_results)\n",
    "best_knn_k = knn_df. loc[knn_df['mean'].idxmax(), 'n_neighbors']\n",
    "display(knn_df[['n_neighbors', 'mean']].rename(columns={'mean': 'Mean Accuracy'}).style.format({'Mean Accuracy': '{:.2%}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=\"1\"><thead><tr><th>max_depth</th><th>Mean Accuracy</th></tr></thead><tbody><tr><td>3</td><td>100.00%</td></tr><tr><td>4</td><td>100.00%</td></tr><tr><td>5</td><td>100.00%</td></tr><tr><td>6</td><td>100.00%</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type":  "display_data"
    }
   ],
   "source": [
    "# Decision Tree tuning (doesn't need scaling)\n",
    "tree_results = []\n",
    "for depth in [3, 4, 5, 6]:\n",
    "    scores = cross_val_score(DecisionTreeClassifier(max_depth=depth, random_state=42), X, y, cv=kfold)\n",
    "    tree_results.append({'max_depth': depth, 'mean':  scores.mean(), 'std': scores. std()})\n",
    "\n",
    "tree_df = pd.DataFrame(tree_results)\n",
    "best_tree_depth = int(tree_df. loc[tree_df['mean'].idxmax(), 'max_depth'])\n",
    "display(tree_df[['max_depth', 'mean']]. rename(columns={'mean': 'Mean Accuracy'}).style.format({'Mean Accuracy': '{:.2%}'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best KNN:  n_neighbors=11.  Best Decision Tree: max_depth=3 (simplest with 100% accuracy).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Model Comparison:  Effect of Preprocessing\n",
    "\n",
    "Comparing KNN performance on raw vs preprocessed data.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=\"1\"><thead><tr><th>Data</th><th>Mean Accuracy</th><th>Std</th></tr></thead><tbody><tr><td>Raw (FP-6)</td><td>91.80%</td><td>±1.47%</td></tr><tr><td>Scaled (FP-7)</td><td>92.15%</td><td>±1.32%</td></tr><tr><td>PCA (4 comp)</td><td>91.95%</td><td>±1.28%</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type":  "display_data"
    }
   ],
   "source": [
    "knn_raw = cross_val_score(KNeighborsClassifier(n_neighbors=11), X, y, cv=kfold)\n",
    "knn_scaled = cross_val_score(KNeighborsClassifier(n_neighbors=11), X_scaled, y, cv=kfold)\n",
    "knn_pca = cross_val_score(KNeighborsClassifier(n_neighbors=11), X_pca, y, cv=kfold)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Data': ['Raw (FP-6)', 'Scaled (FP-7)', 'PCA (4 comp)'],\n",
    "    'Mean Accuracy': [f\"{s. mean()*100:.2f}%\" for s in [knn_raw, knn_scaled, knn_pca]],\n",
    "    'Std':  [f\"±{s.std()*100:.2f}%\" for s in [knn_raw, knn_scaled, knn_pca]]\n",
    "})\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type":  "markdown",
   "metadata": {},
   "source": [
    "Scaling improves KNN accuracy by ~0.35% and reduces variance.  The improvement is modest because feature ranges in this dataset are already fairly similar (mostly 0-10). In datasets with more varied ranges, scaling would have a larger effect.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Feature Importance (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs":  [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth=best_tree_depth, random_state=42)\n",
    "tree_model. fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': tree_model. feature_importances_\n",
    "}).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source":  [
    "def plot_machine1():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt. barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Lifestyle Factor', fontsize=12)\n",
    "    plt.title('Feature Importance:  Which Lifestyle Factors Predict Stress?', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    for i, (feature, importance) in enumerate(zip(importance_df['Feature'], importance_df['Importance'])):\n",
    "        plt.text(importance + 0.01, i, f'{importance*100:.1f}%', va='center', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_machine1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "Study hours (72. 4%) and sleep hours (27.6%) are the only features the Decision Tree uses.  This explains why PCA didn't dramatically improve results—the tree already ignores irrelevant features through its own feature selection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs":  [],
   "source": [
    "# using scaled features for KNN now\n",
    "y_pred_knn = cross_val_predict(KNeighborsClassifier(n_neighbors=int(best_knn_k)), X_scaled, y, cv=kfold)\n",
    "y_pred_tree = cross_val_predict(DecisionTreeClassifier(max_depth=best_tree_depth, random_state=42), X, y, cv=kfold)\n",
    "\n",
    "cm_knn = confusion_matrix(y, y_pred_knn)\n",
    "cm_tree = confusion_matrix(y, y_pred_tree)\n",
    "\n",
    "knn_accuracy = cm_knn. trace() / cm_knn.sum() * 100\n",
    "tree_accuracy = cm_tree.trace() / cm_tree.sum() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs":  [],
   "source": [
    "def plot_machine2():\n",
    "    fig, axes = plt. subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                xticklabels=['Low', 'Moderate', 'High'],\n",
    "                yticklabels=['Low', 'Moderate', 'High'])\n",
    "    axes[0]. set_title(f'KNN (Scaled) - Accuracy: {knn_accuracy:.1f}%', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    sns.heatmap(cm_tree, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "                xticklabels=['Low', 'Moderate', 'High'],\n",
    "                yticklabels=['Low', 'Moderate', 'High'])\n",
    "    axes[1].set_title(f'Decision Tree - Accuracy: {tree_accuracy:.1f}%', fontsize=13, fontweight='bold')\n",
    "    axes[1]. set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_machine2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with scaling:** ~92% accuracy, errors mostly between adjacent stress levels.\n",
    "\n",
    "**Decision Tree:** 100% accuracy.  The dataset has clear thresholds that perfectly separate stress categories.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "## Summary:  FP-6 vs FP-7\n",
    "\n",
    "| Aspect | FP-6 | FP-7 |\n",
    "|--------|------|------|\n",
    "| Data loading | Direct CSV | Import from parse_data. ipynb |\n",
    "| Preprocessing | None | StandardScaler |\n",
    "| Dimensionality reduction | None | PCA tested |\n",
    "| KNN accuracy | ~91.8% | ~92.2% |\n",
    "\n",
    "The preprocessing improvements are modest for this dataset because feature ranges were already similar. The main benefit is demonstrating proper ML workflow as taught in Lessons 13-14."
   ]
  }
 ],
 "metadata": {
  "kernelspec":  {
   "display_name":  "Python 3",
   "language":  "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
