{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f806003-74fb-4371-9eef-d833f35e1621",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "# machine.ipynb\n",
    "\n",
    "This notebook contains **machine learning analysis** to predict student stress levels.  The main notebook (main.ipynb) reports just a summary with two key figures.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "12acba30-5ab7-40a1-81ee-663a211ab7d2",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "## Plan:  Predicting Student Stress Levels\n",
    "\n",
    "I will use **machine learning** to predict whether a student has Low, Moderate, or High stress based on their lifestyle factors.\n",
    "\n",
    "**Research Question:** Can we predict a student's stress level from their daily habits (study hours, sleep, social time, physical activity)?\n",
    "    \n",
    "### Data\n",
    "- **Target variable (y)**: stress (Low=1, Moderate=2, High=3)\n",
    "- **Features (X)**: studyhours, sleephours, socialhours, activityhours, Gender\n",
    "- **Validation**:  **K-fold cross-validation (5 folds)** to evaluate distribution of outcomes\n",
    "\n",
    "### Models\n",
    "- **K-Nearest Neighbors (KNN)** - with hyperparameter tuning for n_neighbors\n",
    "- **Decision Tree** - with hyperparameter tuning for max_depth\n",
    "\n",
    "### Insightful Analysis\n",
    "- **Parameter Tuning**: Finding optimal hyperparameters using cross-validation\n",
    "- **Feature Importance**: Which lifestyle factor is the best predictor for stress?\n",
    "- **Confusion Matrix**: Aggregated across all folds - where does the model make mistakes?\n",
    "- **Model Comparison**: Which model works better and why?\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "ab85726a-7bd2-4743-988e-6b9755790682",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "## Setup and Data Preparation"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count": 1,
   "id": "a5c0f4ca-e78d-4337-978e-b283f96a66c2",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('student_lifestyle_dataset.. csv')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Grades': 'grades',\n",
    "    'Stress_Level': 'stress',\n",
    "    'Study_Hours_Per_Day': 'studyhours',\n",
    "    'Extracurricular_Hours_Per_Day': 'echours',\n",
    "    'Sleep_Hours_Per_Day': 'sleephours',\n",
    "    'Social_Hours_Per_Day': 'socialhours',\n",
    "    'Physical_Activity_Hours_Per_Day': 'activityhours'\n",
    "})\n",
    "\n",
    "# Convert stress levels to numbers\n",
    "df['stress'] = df['stress'].map({'Low': 1, 'Moderate': 2, 'High': 3})\n",
    "\n",
    "# Convert Gender to numbers\n",
    "df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# Features and Target\n",
    "X = df[['studyhours', 'sleephours', 'socialhours', 'activityhours', 'Gender']]\n",
    "y = df['stress']\n",
    "\n",
    "# Set up k-fold cross-validation (5 folds)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Total dataset size: {len(df)} students\")\n",
    "print(f\"\\nStress Level distribution:\")\n",
    "print(df['stress'].value_counts().sort_index())\n",
    "print(f\"\\nStress Level percentages: \")\n",
    "print(df['stress'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(f\"\\nNote: We use k-fold cross-validation (5 folds) instead of a single train/test split. \")\n",
    "print(f\"This means each student will be in the test set exactly once across the 5 folds.\")"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "fc6bd7bb-ea55-4ed8-b2ef-33c419f29e72",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "<br>\n",
    "\n",
    "**Stress Distribution:**\n",
    "The output above shows how many students fall into each stress category. This check matters because if the classes are very imbalanced, the model could just guess the majority class and still look accurate.\n",
    "\n",
    "Our classes are somewhat imbalanced (High stress is most common at 51.4%), but not severely so. We proceed with standard classification.\n",
    "\n",
    "<br>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4939a6f-44f8-429d-b198-8adf40fefe80",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "## Cross-Validation and Parameter Tuning\n",
    "\n",
    "Instead of relying on a single train/test split, we use **k-fold cross-validation** to evaluate models across multiple random splits. We also test different hyperparameters to find the best settings."
   ]
  },
  {
   "cell_type":  "code",
   "execution_count": 2,
   "id": "5e69065f-2eaf-4961-a1ba-d6d552f001e0",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"K-FOLD CROSS-VALIDATION RESULTS (5 folds)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"This evaluates models across 5 different random train/test splits.\")\n",
    "print(\"This shows the DISTRIBUTION of results, not just one outcome.\")\n",
    "print()\n",
    "\n",
    "# KNN with k=5 (default)\n",
    "knn_cv = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_cv_scores = cross_val_score(knn_cv, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"KNN (n_neighbors=5) - Cross-Validation Scores:\")\n",
    "for i, score in enumerate(knn_cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score*100:.2f}%\")\n",
    "print(f\"  Mean Accuracy: {knn_cv_scores.mean()*100:.2f}%\")\n",
    "print(f\"  Std Deviation: {knn_cv_scores.std()*100:.2f}%\")\n",
    "print(f\"  Range: {knn_cv_scores.min()*100:.2f}% - {knn_cv_scores.max()*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Decision Tree with max_depth=5 (default)\n",
    "tree_cv = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_cv_scores = cross_val_score(tree_cv, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"Decision Tree (max_depth=5) - Cross-Validation Scores: \")\n",
    "for i, score in enumerate(tree_cv_scores, 1):\n",
    "    print(f\"  Fold {i}:  {score*100:.2f}%\")\n",
    "print(f\"  Mean Accuracy: {tree_cv_scores. mean()*100:.2f}%\")\n",
    "print(f\"  Std Deviation: {tree_cv_scores.std()*100:.2f}%\")\n",
    "print(f\"  Range:  {tree_cv_scores.min()*100:.2f}% - {tree_cv_scores. max()*100:.2f}%\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count":  3,
   "id":  "f8faba6b-4458-433a-a403-a3b5316891a8",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source":  [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARAMETER TUNING:  Testing Different Hyperparameters\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"We test different parameter values to find the best settings. \")\n",
    "print()\n",
    "\n",
    "# KNN Parameter Tuning\n",
    "knn_neighbors_range = [3, 5, 7, 9, 11, 15]\n",
    "knn_results = []\n",
    "\n",
    "print(\"KNN - Testing different n_neighbors values:\")\n",
    "print(\"-\" * 70)\n",
    "for k in knn_neighbors_range:\n",
    "    knn_model_tuned = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv_scores = cross_val_score(knn_model_tuned, X, y, cv=kfold, scoring='accuracy')\n",
    "    mean_score = cv_scores.mean()\n",
    "    std_score = cv_scores.std()\n",
    "    knn_results.append({'n_neighbors': k, 'mean_accuracy': mean_score, 'std':  std_score})\n",
    "    print(f\"n_neighbors={k: 2d}: Mean Accuracy = {mean_score*100:.2f}% (±{std_score*100:.2f}%)\")\n",
    "\n",
    "best_knn = max(knn_results, key=lambda x: x['mean_accuracy'])\n",
    "print(f\"\\n✓ Best KNN:  n_neighbors={best_knn['n_neighbors']} with {best_knn['mean_accuracy']*100:.2f}% accuracy\")\n",
    "print()\n",
    "\n",
    "# Decision Tree Parameter Tuning\n",
    "tree_depth_range = [3, 4, 5, 6, 7, 8, 10]\n",
    "tree_results = []\n",
    "\n",
    "print(\"Decision Tree - Testing different max_depth values:\")\n",
    "print(\"-\" * 70)\n",
    "for depth in tree_depth_range:\n",
    "    tree_model_tuned = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    cv_scores = cross_val_score(tree_model_tuned, X, y, cv=kfold, scoring='accuracy')\n",
    "    mean_score = cv_scores.mean()\n",
    "    std_score = cv_scores.std()\n",
    "    tree_results. append({'max_depth': depth, 'mean_accuracy': mean_score, 'std': std_score})\n",
    "    print(f\"max_depth={depth:2d}: Mean Accuracy = {mean_score*100:.2f}% (±{std_score*100:.2f}%)\")\n",
    "\n",
    "best_tree = max(tree_results, key=lambda x: x['mean_accuracy'])\n",
    "print(f\"\\n✓ Best Decision Tree: max_depth={best_tree['max_depth']} with {best_tree['mean_accuracy']*100:. 2f}% accuracy\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count":  4,
   "id":  "848265a5-eec1-49d4-8722-18f4621fb31d",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Visualize Parameter Tuning Results\n",
    "knn_df = pd.DataFrame(knn_results)\n",
    "tree_df = pd.DataFrame(tree_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(knn_df['n_neighbors'], knn_df['mean_accuracy']*100, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].fill_between(knn_df['n_neighbors'], \n",
    "                      (knn_df['mean_accuracy'] - knn_df['std'])*100,\n",
    "                      (knn_df['mean_accuracy'] + knn_df['std'])*100,\n",
    "                      alpha=0.2, color='steelblue')\n",
    "axes[0].set_xlabel('n_neighbors', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Validation Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('KNN:  Performance vs n_neighbors', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(knn_df['n_neighbors'])\n",
    "axes[0].axvline(x=best_knn['n_neighbors'], color='red', linestyle='--', alpha=0.7, label=f\"Best: n_neighbors={best_knn['n_neighbors']}\")\n",
    "axes[0]. legend()\n",
    "axes[0].set_ylim([89, 93])\n",
    "\n",
    "axes[1].plot(tree_df['max_depth'], tree_df['mean_accuracy']*100, marker='s', linewidth=2, markersize=8, color='seagreen')\n",
    "axes[1].fill_between(tree_df['max_depth'], \n",
    "                      (tree_df['mean_accuracy'] - tree_df['std'])*100,\n",
    "                      (tree_df['mean_accuracy'] + tree_df['std'])*100,\n",
    "                      alpha=0.2, color='seagreen')\n",
    "axes[1].set_xlabel('max_depth', fontsize=12)\n",
    "axes[1].set_ylabel('Cross-Validation Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Decision Tree: Performance vs max_depth', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(tree_df['max_depth'])\n",
    "axes[1].axvline(x=best_tree['max_depth'], color='red', linestyle='--', alpha=0.7, label=f\"Best: max_depth={best_tree['max_depth']}\")\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([99, 100. 5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights from Parameter Tuning:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"KNN: Best performance at n_neighbors={best_knn['n_neighbors']} ({best_knn['mean_accuracy']*100:.2f}%)\")\n",
    "print(f\"     - Accuracy increases from n=3 to n=11, then slightly decreases\")\n",
    "print(f\"     - Larger k values reduce overfitting but may miss local patterns\")\n",
    "print()\n",
    "print(f\"Decision Tree: All depths (3-10) achieve 100% accuracy\")\n",
    "print(f\"     - max_depth=3 is sufficient and simplest (Occam's Razor)\")\n",
    "print(f\"     - Deeper trees don't improve performance, just add complexity\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acf4e6-7c5a-47b0-99e9-c37f102c3a98",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "## Model Training and Evaluation Using Cross-Validation\n",
    "\n",
    "We now train models with the best hyperparameters found above and evaluate using cross-validation."
   ]
  },
  {
   "cell_type":  "code",
   "execution_count": 5,
   "id": "599601cb-80a9-48aa-961f-8197d474ba6f",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(f\"KNN:  CROSS-VALIDATION EVALUATION (Best:  n_neighbors={best_knn['n_neighbors']})\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_knn['n_neighbors'])\n",
    "knn_cv_results = cross_validate(knn_best, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "print(f\"KNN (n_neighbors={best_knn['n_neighbors']}) - Cross-Validation Results:\")\n",
    "print()\n",
    "for i, (train_score, test_score) in enumerate(zip(knn_cv_results['train_score'], knn_cv_results['test_score']), 1):\n",
    "    print(f\"  Fold {i}: Train = {train_score*100:.2f}%, Test = {test_score*100:.2f}%\")\n",
    "print()\n",
    "print(f\"Mean Test Accuracy: {knn_cv_results['test_score'].mean()*100:.2f}%\")\n",
    "print(f\"Std Deviation: {knn_cv_results['test_score'].std()*100:.2f}%\")\n",
    "print(f\"Range: {knn_cv_results['test_score']. min()*100:.2f}% - {knn_cv_results['test_score'].max()*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Decision Tree: CROSS-VALIDATION EVALUATION (Best: max_depth={best_tree['max_depth']})\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "tree_best = DecisionTreeClassifier(max_depth=best_tree['max_depth'], random_state=42)\n",
    "tree_cv_results = cross_validate(tree_best, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "print(f\"Decision Tree (max_depth={best_tree['max_depth']}) - Cross-Validation Results:\")\n",
    "print()\n",
    "for i, (train_score, test_score) in enumerate(zip(tree_cv_results['train_score'], tree_cv_results['test_score']), 1):\n",
    "    print(f\"  Fold {i}: Train = {train_score*100:.2f}%, Test = {test_score*100:.2f}%\")\n",
    "print()\n",
    "print(f\"Mean Test Accuracy: {tree_cv_results['test_score'].mean()*100:.2f}%\")\n",
    "print(f\"Std Deviation: {tree_cv_results['test_score'].std()*100:.2f}%\")\n",
    "print(f\"Range: {tree_cv_results['test_score'].min()*100:.2f}% - {tree_cv_results['test_score'].max()*100:.2f}%\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166f49e-8a50-46e3-917f-e2cf89a1e616",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "<br>\n",
    "\n",
    "**Cross-Validation Results Summary:**\n",
    "\n",
    "- **KNN (n_neighbors=11)**: Mean accuracy ~91.8% across 5 folds, with small variation (±1.5%)\n",
    "- **Decision Tree (max_depth=3)**: Mean accuracy 100% across all 5 folds, with zero variation\n",
    "\n",
    "The fact that test accuracy is consistent across folds means the results are reliable and not dependent on a lucky/unlucky split.\n",
    "\n",
    "<br>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d954b6d-7326-46c0-ad02-19c5b912ad76",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "### Insightful Analysis #1: Feature Importance of Decision Tree"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count": 6,
   "id": "9c1af7c9-5c40-40f4-acd6-2197f3adb083",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Train the best Decision Tree model on full data to get feature importances\n",
    "tree_model = DecisionTreeClassifier(max_depth=best_tree['max_depth'], random_state=42)\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "feature_importances = tree_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance':  feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print()\n",
    "for idx, row in importance_df.iterrows():\n",
    "    print(f\"{row['Feature']: 20s}:  {row['Importance']:.4f} ({row['Importance']*100:. 2f}%)\")\n",
    "print()\n",
    "\n",
    "# Define plot function for main. ipynb\n",
    "def plot_machine1():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Lifestyle Factor', fontsize=12)\n",
    "    plt.title('Feature Importance:  Which Lifestyle Factors Predict Stress?', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Percentage labels\n",
    "    for i, (feature, importance) in enumerate(zip(importance_df['Feature'], importance_df['Importance'])):\n",
    "        plt.text(importance + 0.01, i, f'{importance*100:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_machine1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae65dd-331e-44f2-9492-5ee6f237576d",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "<br>\n",
    "\n",
    "The importance of features in our model is:\n",
    "\n",
    "1. **Study Hours (72.4%)** → most important factor by far\n",
    "2. **Sleep Hours (27.6%)** → second most important factor\n",
    "3. **Social Hours, Physical Activity, Gender (0%)** → other factors have no impact on the model\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "How much a student studies is the biggest driver of their stress level. Sleep hours also matter, but a lot less.\n",
    "\n",
    "Surprisingly, social time, exercise, and gender have no importance in this model.  This does not mean they are unrelated to stress in real life, but that once you know a student's study hours and sleep hours, knowing their social time or exercise habits doesn't help predict stress any better.\n",
    "\n",
    "**Insightful Take-away**\n",
    "\n",
    "A simple analysis could just say: \"Study hours is important because it predicts 72.4% of the stress level.\"\n",
    "\n",
    "Here, we find that study hours (and sleep hours) dominates stress prediction so strongly that other lifestyle factors like exercise and social time become irrelevant.  \n",
    "This could suggest that academic pressure is the overwhelming source of stress for students.  In that case, interventions should focus primarily on rest rather than promoting exercise or social activities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "1c11a81c-5ae5-4a34-8571-5e8bb9693e64",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "### Insightful Analysis #2: Confusion Matrix Results (Aggregated Across All Folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count":  7,
   "id":  "38f2060d-fed5-4440-ba00-cad74825dd68",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Compute aggregated confusion matrices using cross_val_predict\n",
    "# This uses all 5 folds combined\n",
    "\n",
    "knn_best_model = KNeighborsClassifier(n_neighbors=best_knn['n_neighbors'])\n",
    "y_pred_knn_cv = cross_val_predict(knn_best_model, X, y, cv=kfold)\n",
    "cm_knn = confusion_matrix(y, y_pred_knn_cv)\n",
    "\n",
    "tree_best_model = DecisionTreeClassifier(max_depth=best_tree['max_depth'], random_state=42)\n",
    "y_pred_tree_cv = cross_val_predict(tree_best_model, X, y, cv=kfold)\n",
    "cm_tree = confusion_matrix(y, y_pred_tree_cv)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFUSION MATRICES (Aggregated Across All 5 Folds)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"These matrices show predictions for ALL 2000 students. \")\n",
    "print(\"Each student was predicted once (when they were in the test fold).\")\n",
    "print()\n",
    "\n",
    "# Calculate accuracies\n",
    "knn_accuracy = cm_knn.trace() / cm_knn.sum() * 100\n",
    "tree_accuracy = cm_tree.trace() / cm_tree.sum() * 100\n",
    "\n",
    "# Define plot function for main.ipynb\n",
    "def plot_machine2():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # KNN\n",
    "    sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Low', 'Moderate', 'High'],\n",
    "            yticklabels=['Low', 'Moderate', 'High'])\n",
    "    axes[0].set_title(f'KNN Confusion Matrix (All 5 Folds)\\n(Accuracy:  {knn_accuracy:.1f}%)', fontsize=13, fontweight='bold')\n",
    "    axes[0]. set_xlabel('Predicted Stress Level', fontsize=11)\n",
    "    axes[0].set_ylabel('Actual Stress Level', fontsize=11)\n",
    "\n",
    "    # Decision Tree\n",
    "    sns.heatmap(cm_tree, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Low', 'Moderate', 'High'],\n",
    "            yticklabels=['Low', 'Moderate', 'High'])\n",
    "    axes[1].set_title(f'Decision Tree Confusion Matrix (All 5 Folds)\\n(Accuracy: {tree_accuracy:.1f}%)', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Predicted Stress Level', fontsize=11)\n",
    "    axes[1].set_ylabel('Actual Stress Level', fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_machine2()\n",
    "\n",
    "# Error analysis\n",
    "print()\n",
    "print(\"KNN Error Analysis (across all folds):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {cm_knn.sum()}\")\n",
    "print(f\"Correct predictions: {cm_knn. trace()}\")\n",
    "print(f\"Incorrect predictions: {cm_knn.sum() - cm_knn.trace()}\")\n",
    "print(f\"\\nWhere does KNN make mistakes?\")\n",
    "stress_names = ['Low', 'Moderate', 'High']\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i != j and cm_knn[i, j] > 0:\n",
    "            print(f\"  - Predicted {stress_names[j]} when actual was {stress_names[i]}: {cm_knn[i, j]} times\")\n",
    "\n",
    "print()\n",
    "print(\"Decision Tree Error Analysis (across all folds):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {cm_tree.sum()}\")\n",
    "print(f\"Correct predictions: {cm_tree.trace()}\")\n",
    "print(f\"Incorrect predictions: {cm_tree. sum() - cm_tree.trace()}\")\n",
    "if cm_tree.sum() - cm_tree.trace() == 0:\n",
    "    print(\"\\n  Decision Tree made NO mistakes across all 5 folds!\")"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "36db4b02-2f81-4caf-bd47-8fe7202c38a1",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "<br>\n",
    "\n",
    "**Confusion Matrix Analysis (Aggregated Across All Folds)**\n",
    "\n",
    "A confusion matrix shows where the model makes mistakes. The diagonal shows correct predictions, off-diagonal shows errors.\n",
    "\n",
    "**KNN Error Pattern (~164 mistakes out of 2000 = ~8.2% error rate):**\n",
    "\n",
    "Looking at the KNN confusion matrix:\n",
    "- **Predicted Moderate when actual was Low**: 44 times\n",
    "- **Predicted High when actual was Low**: 13 times  \n",
    "- **Predicted Low when actual was Moderate**: 25 times\n",
    "- **Predicted High when actual was Moderate**: 55 times\n",
    "- **Predicted Low when actual was High**: 4 times\n",
    "- **Predicted Moderate when actual was High**: 23 times\n",
    "\n",
    "**The Pattern:** KNN's biggest problem is confusing **Moderate stress with High stress** (55 errors) and **Low with Moderate** (44 errors). This makes sense because Moderate is the \"middle\" category that borders both Low and High.\n",
    "\n",
    "**Decision Tree:  Perfect Classification (0 mistakes across all folds)**\n",
    "\n",
    "The Decision Tree achieved 100% accuracy across ALL folds because this dataset has **very clear, deterministic rules**. The tree learned thresholds like:\n",
    "- \"If studyhours > X AND sleephours < Y, then High stress\"\n",
    "\n",
    "These rules perfectly separate the three stress categories in every fold.\n",
    "\n",
    "**Insightful Take-away**\n",
    "\n",
    "A simple analysis would say: \"Decision Tree is better because it has 100% accuracy.\"\n",
    "\n",
    "An insightful analysis says: \"The Decision Tree's perfect accuracy across ALL 5 folds reveals that stress levels in this dataset follow deterministic rules based on study and sleep hours.  This is not overfitting to one lucky split—it's a consistent pattern.  KNN struggles at class boundaries because it averages neighbors rather than learning explicit thresholds.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "dc805f71-ab50-4ecc-a360-c17c21144d97",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "### Insightful Analysis #3: Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a31bb8-4767-4fe9-9116-aea7aead8b38",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Model comparison summary\n",
    "knn_mean = knn_cv_results['test_score'].mean()\n",
    "knn_std = knn_cv_results['test_score'].std()\n",
    "tree_mean = tree_cv_results['test_score'].mean()\n",
    "tree_std = tree_cv_results['test_score'].std()\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['KNN (n_neighbors=11)', 'Decision Tree (max_depth=3)'],\n",
    "    'Mean CV Accuracy': [f\"{knn_mean*100:.2f}%\", f\"{tree_mean*100:.2f}%\"],\n",
    "    'Std Deviation': [f\"±{knn_std*100:.2f}%\", f\"±{tree_std*100:.2f}%\"],\n",
    "    'Total Errors': [cm_knn.sum() - cm_knn.trace(), cm_tree.sum() - cm_tree.trace()],\n",
    "    'Best Parameter': [f\"n_neighbors={best_knn['n_neighbors']}\", f\"max_depth={best_tree['max_depth']}\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = ['KNN', 'Decision Tree']\n",
    "mean_accs = [knn_mean * 100, tree_mean * 100]\n",
    "std_accs = [knn_std * 100, tree_std * 100]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.5\n",
    "\n",
    "bars = axes[0].bar(x, mean_accs, width, yerr=std_accs, capsize=5, color=['steelblue', 'seagreen'])\n",
    "axes[0].set_ylabel('Cross-Validation Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy Comparison (5-Fold CV)', fontsize=13, fontweight='bold')\n",
    "axes[0]. set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].set_ylim([85, 102])\n",
    "\n",
    "for bar, acc in zip(bars, mean_accs):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1. 5, f'{acc:.1f}%', \n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "errors = [cm_knn.sum() - cm_knn.trace(), cm_tree.sum() - cm_tree.trace()]\n",
    "colors = ['#ff6b6b', '#51cf66']\n",
    "axes[1].bar(models, errors, color=colors)\n",
    "axes[1].set_ylabel('Number of Errors', fontsize=12)\n",
    "axes[1].set_title('Prediction Errors (All 2000 samples across 5 folds)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim([0, max(errors) + 20])\n",
    "\n",
    "for i, (model, error) in enumerate(zip(models, errors)):\n",
    "    axes[1].text(i, error + 5, str(error), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type":  "markdown",
   "id":  "0442d4b2-eca5-4e71-bd0e-10c8a0ced703",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "<br>\n",
    "\n",
    "**Performance Summary (5-Fold Cross-Validation):**\n",
    "\n",
    "| Metric | KNN (n=11) | Decision Tree (depth=3) | Winner |\n",
    "|--------|------------|-------------------------|--------|\n",
    "| Mean CV Accuracy | ~91.8% | 100.0% | Decision Tree |\n",
    "| Std Deviation | ~±1.5% | ±0.0% | Decision Tree |\n",
    "| Total Errors | ~164 | 0 | Decision Tree |\n",
    "\n",
    "**Why Decision Tree Performs Better:**\n",
    "\n",
    "1. **Clear Decision Boundaries**: This dataset has very clear thresholds.  For example, there's probably a specific study hours cutoff above which students experience High stress.  Decision Trees find these exact thresholds.\n",
    "\n",
    "2. **KNN's Weakness**: KNN averages the nearest neighbors.  If a student is right at the boundary between Moderate and High stress, their neighbors might be mixed, leading to errors.\n",
    "\n",
    "3. **Interpretability**: Decision Trees provide actionable rules like \"Students who study >X hours AND sleep <Y hours have High stress. \" University counselors could use these rules to identify at-risk students.\n",
    "\n",
    "**Insightful Take-away**\n",
    "\n",
    "A simple analysis would say: \"Decision Tree is better because it has higher accuracy (100% vs ~91.8%).\"\n",
    "\n",
    "An insightful analysis says: \"The Decision Tree's perfect accuracy across all 5 cross-validation folds reveals fundamental properties of this dataset—stress follows deterministic rules based on study and sleep hours. The ~8% error rate of KNN at class boundaries indicates that these boundaries are sharp thresholds rather than gradual transitions. This finding is robust across different random train/test splits.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer":  "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor":  5
}
